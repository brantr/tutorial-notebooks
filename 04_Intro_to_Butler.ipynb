{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img align=\"left\" src = https://project.lsst.org/sites/default/files/Rubin-O-Logo_0.png width=250> \n",
    "<b>Introduction to the LSST data Butler</b> <br>\n",
    "Last verified to run on <b>TBD</b> with LSST Science Pipelines release <b>TBD</b> <br>\n",
    "Contact author: Alex Drlica-Wagner <br>\n",
    "Credit: Originally developed by Alex Drlica-Wagner in the context of the LSST Stack Club <br>\n",
    "Target audience: All DP0 delegates. <br>\n",
    "Container Size: medium <br>\n",
    "Questions welcome at <a href=\"https://community.lsst.org/c/support/dp0\">community.lsst.org/c/support/dp0</a> <br>\n",
    "Find DP0 documentation and resources at <a href=\"https://dp0-1.lsst.io\">dp0-1.lsst.io</a> <br>\n",
    "\n",
    "<br>\n",
    "This notebook provides an introduction to the use of the data Butler. The Butler is the LSST Science Pipelines interface for managing, reading, and writing datasets. The Butler can be used to explore the contents of the DP0.1 data repository and access the DP0.1 data. The current version of the Butler (referred to as \"Gen-3\") is still under development, and this notebook may be modified in the future.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "The goals of this notebook are to:<br>\n",
    "1. create an instance of the Butler<br>\n",
    "2. explore the DP0.1 data repository<br>\n",
    "3. retrive and display some image and catalog data<br>\n",
    "4. create in image cutout at a user-specified coordinate<br>\n",
    "5. retrieve and plot catalog data \n",
    "\n",
    "\n",
    "\n",
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This should match the verified version listed at the start of the notebook\n",
    "! eups list -s lsst_distrib"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Create the Butler\n",
    "\n",
    "In this section we are going to create an instance of the Butler. We start with some general python package imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generic imports\n",
    "import os,glob\n",
    "import pylab as plt\n",
    "plt.rcParams['figure.figsize'] = (8.0, 8.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We import several packages from the LSST Science Pipelines. \n",
    "The first import gives us access to the Butler, while the second provides tools for displaying data.\n",
    "\n",
    "More details and techniques regarding image display can be found in the `rubin-dp0` GitHub Organization's [tutorial-notebooks](https://github.com/rubin-dp0/tutorial-notebooks) repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stack imports\n",
    "import lsst.daf.butler as dafButler\n",
    "import lsst.afw.display as afwDisplay\n",
    "afwDisplay.setDefaultBackend('matplotlib')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create the Butler, we need to provide it with a path to the data set, which is called a \"data repository\". Butler repositories can be remote (i.e., pointing to an S3 bucket) or local (i.e., pointing to a directory on the local file system). In this case, we point to an S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "repo='s3://butler-us-central1-dp01'\n",
    "butler = dafButler.Butler(repo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Explore a data repository\n",
    "\n",
    "Now that we've created an instance of the Butler, we can access the data `registry` (a database containing information about available data products). The registry will help us examine what collections of data products exist."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "registry = butler.registry\n",
    "\n",
    "# We can examine the registry with\n",
    "#help(registry)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `registry` is a good tool for investigating a repository (more on the registry schema can be found [here](https://dmtn-073.lsst.io/)). For example, we can get a list of all collections, with"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for c in sorted(registry.queryCollections()):\n",
    "    print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is our first glimpse at the data contained in the repository, but it doesn't teach us *which* collection we are actually interested in. The names do give us some hints though...\n",
    "\n",
    "* `2.2i` - refers to the processing run of the LSST DESC DC2 data (the `i` stands for `imSim`)\n",
    "* `calib` - refers to calibration products that are used for instrument signature removal\n",
    "* `runs` - refers to processed data products\n",
    "* `refcats` - refers to the reference catalogs used for astrometric and photometric calibration\n",
    "* `skymaps` - are the geometric representations of the sky coverage\n",
    "\n",
    "Collections can be nested, so we can access to everything for DC2 Run 2.2i (the primary DP0.1 data set) by selecting the collection `2.2i/runs/DP0.1`. This is a pointer to other collections that expand out recursively... More on collections can be found here: https://pipelines.lsst.io/v/weekly/modules/lsst.daf.butler/organizing.html#collections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this collection is a pointer to other collections, expand those out recursively.\n",
    "collection=\"2.2i/runs/DP0.1\"\n",
    "print(collection)\n",
    "for c in sorted(registry.queryCollections(collection,flattenChains=True)):\n",
    "    print(c, registry.getCollectionType(c))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now create a new Butler instance specifying that we are specifically interested in the `2.2i/runs/DP0.1` data collection. For most uses, this will be the line you will use to create a Butler to work on DP0.1, since you now know that this collection exists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new butler with the collection of interest\n",
    "butler = dafButler.Butler(repo,collections=collection)\n",
    "registry = butler.registry"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You are probably interested in figuring out what kind of data is present in DP0.1. The LSST Science Pipelines classify different data products in terms of `DatasetTypes`. However, individual `DatasetTypes` are defined globally, and thus don't belong to a specific collection. Thus, a query of DatasetTypes will return all DatasetTypes belonging to the repository, and not all of them may belong to the collection of interest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in sorted(registry.queryDatasetTypes()):\n",
    "    print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this list may seem a bit overwhelming, but we can extract some information from the names of the dataset types:\n",
    "\n",
    "- `calexp` - Refers to individual calibrated exposure\n",
    "- `deepCoadd` - Refers to products produced on the coadd images (both images and and source catalogs)\n",
    "- `src` - refers the the catalog of sources\n",
    "- `skyMap` - refers to geometric representations of the sky coverage\n",
    "\n",
    "<b> Which data sets are most appropriate for DP0.1? </b><br>\n",
    "Most DP0.1 delegates will only be interested in data sets with types `ExposureF` or `SourceCatalog`. \n",
    "For images, stick to the `calexp` (processed visit images, or PVIs) and `deepCoadd` (stacked PVIs).\n",
    "For catalogs, the `src` should be used with the `calexp` images, and the `deepCoadd_forced_src` are the most appropriate to be used with the `coadds`.\n",
    "More information can be found in the DP0.1 Data Products Definitions Document (DPDD) at [dp0-1.lsst.io](http://dp0-1.lsst.io).\n",
    "\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We access specific data sets through a set of specifications known as a data identifier (`dataId`). Each `DatasetType` can be identified with a different set of properties, so it is important to be able to determine what properties need to be specified to access data of a specific type. It is possible to get all `DatasetRef` (which include the `dataId`) for a specific `datasetType` in a specific collection with a query like this. Note that this doesn't necessarily guarentee that the specific data set exists, so we include a check that the data set has a valid Uniform Resource Identifier (URI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasetRefs = registry.queryDatasets(datasetType='calexp',collections=collection)\n",
    "for i,ref in enumerate(datasetRefs):\n",
    "    print(ref.dataId.full)\n",
    "    try: uri = butler.getURI(ref)\n",
    "    except: print(\"File not found...\")\n",
    "    if i > 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can get the path from the URI that is returned by `butler.getURI`. Note that this URI does not refer to a local path on the filesystem. We do not need to know exactly where the data live in order to access it. That's the power of the Butler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Repo path: \",repo)\n",
    "uri = butler.getURI(ref)\n",
    "print(\"File URI: \",uri)\n",
    "!ls -lh {uri.ospath}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now say we want to restrict our selection to datasets associated with a specific filter. We can add that constraint to our query, but first we need to figure out what the filters are called... Looking at the dataId object, we see the attributes `physical_filter` and `band` look promising."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ref.dataId.full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"physical_filter = {ref.dataId['physical_filter']}\")\n",
    "print(f\"band = {ref.dataId['band']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It looks like `band` is what we want, so we put it in the `where` argument of `queryDatasets`. Let's try the $g$ band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also sub-select on specific properties of a data set\n",
    "datasetRefs = registry.queryDatasets(datasetType='calexp',dataId={'band': 'g'}, where='visit > 700000', collections=collection)\n",
    "for i,ref in enumerate(datasetRefs):\n",
    "    print(ref.dataId.full)\n",
    "    try: uri = butler.getURI(ref)\n",
    "    except: print(\"File not found...\")\n",
    "    if i > 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Retrieve and plot a calexp with sources\n",
    "\n",
    "Ok, now we have all the information we need to ask the Butler to get a specific data product. We have identified a collection (`2.2i/runs/DP0.1`), a `datasetType` (`calexp`), and the `dataId` (from the `datasetRef`) to uniquely specify an instance of this data set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the list above, let's choose one detector from one visit and ask the Butler to get a `calexp` for us."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could pass the datasetRef that we found above, but since the query may \n",
    "# return results in a different order we define the dataId explicitly for reproducibility. \n",
    "dataId = {'visit': '703697', 'detector': 80, 'band':'g'}\n",
    "calexp = butler.get('calexp',dataId=dataId)\n",
    "\n",
    "# This will print a warning related to the gen2 to gen3 Butler conversion that was performed on DP0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `calexp` is a calibrated CCD from a single exposure. We'll use the afwDisplay interface to show the pixel values and mask plane (more on afwDisplay can be found in other notebooks)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "display = afwDisplay.Display()\n",
    "display.scale('linear', 'zscale')\n",
    "display.mtv(calexp)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a calibrated image, we may want to get the catlog of sources that were extracted from it. The `src` catalog associated with this `calexp` we pass the `dataId` to the butler with the `src` datasetType. Note that this performs another query to the registry database to find the `src` catalog that matches our dataId requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We could get the src table using the dataId as we did above for the calexp, \n",
    "# but this would require the butler to perform another query of the database. \n",
    "# Instead, we can just pass the ref itself directly to butler.get\n",
    "src = butler.get('src',dataId)\n",
    "src = src.copy(True)\n",
    "src.asAstropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the `calexp` with the `src` catalog overlaid. We leave an investigation of this image as an exercise to the user :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And plot!\n",
    "fig = plt.figure()\n",
    "afw_display = afwDisplay.Display(1)\n",
    "afw_display.scale('asinh', 'zscale')\n",
    "afw_display.mtv(calexp)\n",
    "plt.gca().axis('off')\n",
    "\n",
    "with afw_display.Buffering():\n",
    "    for s in src:\n",
    "        afw_display.dot('+', s.getX(), s.getY(), ctype=afwDisplay.RED)\n",
    "        afw_display.dot('o', s.getX(), s.getY(), size=20, ctype='orange') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. How to query for multiple data sets\n",
    "\n",
    "In the case above, both the `calexp` and `src` can be found by the registry, but this will not always necessarily be the case. The `queryDimensions` method provides a more flexible way to query for multiple datasets (requiring an instance of all datasets to be available for that dataId) or ask for different dataId keys than what is used to identify the dataset (which invokes various built-in relationships). An example of this is provided below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use queryDimensions to provide more flexible access\n",
    "dataIds = registry.queryDataIds([\"visit\", \"detector\", \"band\"], datasets=[\"calexp\",\"src\"], where='visit = 703697',\n",
    "                                collections=collection)\n",
    "for i,dataId in enumerate(dataIds):\n",
    "    print(dataId.full)\n",
    "    if i > 10: break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use queryDataIds to grab the dataIds for a subset\n",
    "dataIds = registry.queryDataIds([\"visit\", \"detector\"], datasets=[\"calexp\",\"src\"], \n",
    "                                where=\"band='g' and detector=0 and visit > 700000\",\n",
    "                                collections=collection)\n",
    "for i,dataId in enumerate(dataIds):\n",
    "    print(dataId.full)\n",
    "    if i > 10: break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also get more metadata about a data product dimension. (Note that the record for an exposure and a visit are different.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for dim in ['exposure','visit','detector']:\n",
    "    print(list(registry.queryDimensionRecords(dim, where='visit = 971990 and detector=0'))[0])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Generate an Image Cutout\n",
    "\n",
    "Say we want to grab a cutout of the DP0.1 coadded images at a specific location. In order to do this, we need a few other packages from the LSST Science Pipelines. In particular, access to the geometry and coordinate packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lsst.geom as geom\n",
    "import lsst.afw.coord as afwCoord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutout_coadd(butler, ra, dec, band='r', datasetType='deepCoadd',\n",
    "                 skymap=None, cutoutSideLength=51, **kwargs):\n",
    "    \"\"\"\n",
    "    Produce a cutout from a coadd at the given afw SpherePoint radec position.\n",
    "\n",
    "    Adapted from DC2 tutorial notebook by Michael Wood-Vasey.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    butler: lsst.daf.persistence.Butler\n",
    "        Servant providing access to a data repository\n",
    "    ra: float\n",
    "        Right ascension of the center of the cutout, degrees\n",
    "    dec: float\n",
    "        Declination of the center of the cutout, degrees\n",
    "    filter: string \n",
    "        Filter of the image to load\n",
    "    datasetType: string ['deepCoadd']  \n",
    "        Which type of coadd to load.  Doesn't support 'calexp'\n",
    "    skymap: lsst.afw.skyMap.SkyMap [optional] \n",
    "        Pass in to avoid the Butler read.  Useful if you have lots of them.\n",
    "    cutoutSideLength: float [optional] \n",
    "        Side of the cutout region in pixels.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    MaskedImage\n",
    "    \"\"\"\n",
    "    radec = geom.SpherePoint(ra, dec, geom.degrees)\n",
    "    cutoutSize = geom.ExtentI(cutoutSideLength, cutoutSideLength)\n",
    "\n",
    "    if skymap is None:\n",
    "        skymap = butler.get(\"skyMap\")\n",
    "    \n",
    "    # Look up the tract, patch for the RA, Dec\n",
    "    tractInfo = skymap.findTract(radec)\n",
    "    patchInfo = tractInfo.findPatch(radec)\n",
    "    xy = geom.PointI(tractInfo.getWcs().skyToPixel(radec))\n",
    "    bbox = geom.BoxI(xy - cutoutSize//2, cutoutSize)\n",
    "    patch = tractInfo.getSequentialPatchIndex(patchInfo)\n",
    "\n",
    "    coaddId = {'tract': tractInfo.getId(), 'patch': patch, 'band': band}\n",
    "    parameters = {'bbox':bbox}\n",
    "    \n",
    "    cutout_image = butler.get(datasetType, parameters=parameters, immediate=True, dataId=coaddId)\n",
    "    \n",
    "    return cutout_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ra,dec = 55.064, -29.783 # center of the DC2 region\n",
    "cutout_image = cutout_coadd(butler,ra,dec,datasetType='deepCoadd', cutoutSideLength=201)\n",
    "print(\"The size of the cutout is: \",cutout_image.image.array.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "afw_display = afwDisplay.Display(1)\n",
    "afw_display.scale('asinh', 'zscale')\n",
    "afw_display.mtv(cutout_image.image)\n",
    "plt.gca().axis('off')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Retrieve and plot catalog data from the Butler\n",
    "\n",
    "The TAP service is the recommended way to retrieve DP0.1 catalog data for a notebook, and there are several other tutuorials that demonstrate how to use the TAP service.\n",
    "\n",
    "But if Butler access to catalog data is needed, an easy way to start is by retrieving only the schema data set for a Butler catalog, which can be done without specifying the ``dataId``. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema_coadd_src = butler.get('deepCoadd_forced_src_schema')\n",
    "schema_coadd_src.asAstropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each of the following lines will print the schema to the screen in different ways."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# schema_coadd_src.schema\n",
    "# schema_coadd_src.schema.getNames()\n",
    "# schema_coadd_src.schema.getOrderedNames()\n",
    "print('Number of columns in this table = ', len(schema_coadd_src.schema.getNames()) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps you want to search for all schema elements that contain the term 'psf'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define an array that is all of the column names\n",
    "all_names = schema_coadd_src.schema.getOrderedNames()\n",
    "\n",
    "# Loop over the names and look for the term 'psf'\n",
    "for i,name in enumerate(all_names):\n",
    "    if name.find('psf') >= 0:\n",
    "        print( i, name )\n",
    "del all_names"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probably you will want to know more about the values in these columns. You can do that by printing the documentation string in the schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Turn the schema into a python dictionary, to be able to call a column by name\n",
    "schema_dict = schema_coadd_src.schema.extract('*')\n",
    "\n",
    "# Print the associated docstring for each of the named columns of interest\n",
    "for name in ['base_SdssShape_psf_xx','base_SdssShape_psf_yy','base_SdssShape_psf_xy']:\n",
    "    doc = schema_dict[name].getField().getDoc()\n",
    "    print(name, ' = ', doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Refer to the DP0.1 Data Products Definitions Document (DPDD) at [dp0-1.lsst.io](http://dp0-1.lsst.io) to find out more about the columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "The catalogs are very large and it is not feasible to try and retrieve it in its entirety.\n",
    "Instead, in this example we identify the tract and patch of interest and retrieve only catalog data for a small region of sky.\n",
    "Use the same ra and dec coordinates as above to find the patch and tract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "radec = geom.SpherePoint(ra, dec, geom.degrees)\n",
    "\n",
    "skymap = butler.get(\"skyMap\")\n",
    "\n",
    "tractInfo = skymap.findTract(radec)\n",
    "tract = tractInfo.getId()\n",
    "\n",
    "patchInfo = tractInfo.findPatch(radec)\n",
    "patch = tractInfo.getSequentialPatchIndex(patchInfo)\n",
    "\n",
    "print(tract, patch)\n",
    "\n",
    "coaddId = {'tract': tract, 'patch': patch, 'band':'i'}\n",
    "\n",
    "coadd_src = butler.get('deepCoadd_forced_src',coaddId)\n",
    "coadd_src = src.copy(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the table contents if desired\n",
    "# coadd_src.asAstropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "Convert to a Pandas dataframe (see the first tutorial) for easy interaction.\n",
    "The following cells offer options for printing the column names or the data values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = coadd_src.asAstropy().to_pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for col in data.columns:\n",
    "#     print(col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data['coord_ra'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the locations of sources in the patch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "plt.plot( data['coord_ra'].values, data['coord_dec'].values, 'o', ms=2, alpha=0.5 )\n",
    "plt.xlabel('RA')\n",
    "plt.ylabel('Dec')\n",
    "plt.title('Butler coadd_forced_src objects in tract 4638 patch 43')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LSST",
   "language": "python",
   "name": "lsst"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
